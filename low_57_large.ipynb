{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#存默一贴摁卡，图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "label_csv = None\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "if exists(label_csv):\n",
    "    df_label = pd.read_excel(label_csv, index_col = \"pathid\")\n",
    "pathid = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "path = '/data2/patho-vit_5_23_ccsurv/patient/low'\n",
    "pathid_orig = [f[:-3] for f in os.listdir(os.path.join(path, \"store_5_96\")) if not f.startswith(\".\")]\n",
    "\n",
    "for i, id in enumerate(pathid_orig):\n",
    "    pathid.append(id)\n",
    "    images.append(os.path.join(path, \"store_5_96\", id+\".db\"))\n",
    "    if exists(label_csv):\n",
    "        labels.append(df_label.loc[id][\"labels\"])\n",
    "    else:\n",
    "        labels.append(1e-6)\n",
    "            \n",
    "\n",
    "c = {\"pathid\": pathid, \"images\": images, \"labels\": labels}\n",
    "df = pd.DataFrame(c)\n",
    "\n",
    "df.to_csv(\"files_label/store_5_96.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "torch.save(df.to_dict(orient='list'), \"files_label/store_5_96.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import copy\n",
    "from functools import wraps, partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from einops import repeat, rearrange \n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"/data2/patho-vit_5_23_ccsurv/patient/low\"\n",
    "# 此处patho_vit后的_5_23为包的版本号\n",
    "# 若此jupyternotebook运行中kernel挂掉，重启后仅需运行此一代码块，然后跳到需要运行的代码块即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二层自监督"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from multiprocessing import Value\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "import torch\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class MaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size = (96, 96),      # input_size = (96, 96)\n",
    "        patch_size = 2,              # patch_size = 4\n",
    "        enc_mask_scale = (0.85, 1.0), # 意思是从原图片上切出多大范围的内容图块，这里是取0.85到1.0之间的一个随机数\n",
    "        #pred_mask_scale = (0.15, 0.2), # 意思是从原图片上切出多大范围的目标图块，这里是取0.15到0.2之间的一个随机数\n",
    "        pred_mask_scale = (0.01, 0.01),\n",
    "        aspect_ratio = (0.75, 1.5),   # 意思是目标图像的宽高比\n",
    "        nenc = 1,                     # 几个内容图块\n",
    "        npred = 4,                    # 几个目标图块\n",
    "        min_keep = 4,                 # 至少几个目标图块\n",
    "        allow_overlap = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  # minimum number of patches to keep\n",
    "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        # -- Sample block scale\n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "        # -- Sample block aspect-ratio\n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        # -- Compute block height and width (given scale and aspect-ratio)\n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "        # --\n",
    "        # -- Loop to sample masks until we find a valid one\n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            # -- Sample block top-left corner\n",
    "            top = torch.randint(0, self.height - h, (1,))\n",
    "            left = torch.randint(0, self.width - w, (1,))\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            # -- Constrain mask to a set of acceptable regions\n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "            # -- If mask too small try again\n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        B = len(batch)\n",
    "\n",
    "        collated_batch = default_collate(batch)\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions= None\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_vit.airport3 import layer2dataset\n",
    "\n",
    "train_lib = \"/data2/patho-vit_5_23_ccsurv/patient/low/files_label/store_5_96.db\"\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = layer2dataset(train_lib, transform = transform, subsample = 96)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    collate_fn = MaskCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_mask(x, mask):\n",
    "    all_x = []\n",
    "    for m in mask:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def loss_fn(z, h):\n",
    "    loss = F.smooth_l1_loss(z, h)\n",
    "    loss = loss.sum(dim = -1).mean()\n",
    "    return loss\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 12, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_encoder(nn.Module):\n",
    "    def __init__(self, *, image_size = 96, patch_size = 2, channels = 768, emb_dropout = 0., \n",
    "                 dim = 768 * 2, depth = 35, heads = 12 * 2, dim_head = 64, mlp_dim = 768 * 2 * 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        device = img.device\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = apply_mask(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, *, num_patches = 48 * 48, encoder_dim = 768 * 2, \n",
    "                 decoder_dim = 768 * 2, decoder_depth = 8, decoder_heads = 12 * 2, decoder_dim_head = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.black_token = nn.Parameter(torch.randn(1, 1, decoder_dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "\n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        device = x.device\n",
    "        batch, num_white, _ = x.shape\n",
    "        \n",
    "        white_tokens = self.enc_to_dec(x)\n",
    "        white_tokens += self.decoder_pos_emb(context_mask[0])\n",
    "        \n",
    "        black_tokens1 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[0].size(-1))\n",
    "        black_tokens1 = black_tokens1 + self.decoder_pos_emb(target_mask[0])\n",
    "        \n",
    "        black_tokens2 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[1].size(-1))\n",
    "        black_tokens2 = black_tokens2 + self.decoder_pos_emb(target_mask[1])\n",
    "        \n",
    "        black_tokens3 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[2].size(-1))\n",
    "        black_tokens3 = black_tokens3 + self.decoder_pos_emb(target_mask[2])\n",
    "        \n",
    "        black_tokens4 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[3].size(-1))\n",
    "        black_tokens4 = black_tokens4 + self.decoder_pos_emb(target_mask[3])\n",
    "        \n",
    "        decoder_tokens = torch.zeros(batch, self.num_patches, self.decoder_dim, device=device)\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        decoder_tokens[batch_range, context_mask[0]] = white_tokens\n",
    "        decoder_tokens[batch_range, target_mask[0]] = black_tokens1\n",
    "        decoder_tokens[batch_range, target_mask[1]] = black_tokens2\n",
    "        decoder_tokens[batch_range, target_mask[2]] = black_tokens3\n",
    "        decoder_tokens[batch_range, target_mask[3]] = black_tokens4\n",
    "        \n",
    "        decoder_tokens = self.decoder(decoder_tokens)\n",
    "        \n",
    "        context = apply_mask(decoder_tokens, target_mask)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jepa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_encoder = Context_encoder()\n",
    "        self.predictor = Predictor()\n",
    "        #self.target_encoder = Target_encoder()\n",
    "        self.target_encoder = None\n",
    "        \n",
    "        device = get_module_device(Context_encoder())\n",
    "        self.to(device)\n",
    "        \n",
    "        self.forward(torch.randn(2, 768, 96, 96, device = device), torch.ones(2, 48 * 48, dtype  = torch.int64), [torch.ones(2, 48 * 48, dtype  = torch.int64), \n",
    "                                                                                                             torch.ones(2, 48 * 48, dtype  = torch.int64), \n",
    "                                                                                                              torch.ones(2, 48 * 48, dtype  = torch.int64),                                     \n",
    "                                                                                                              torch.ones(2, 48 * 48, dtype  = torch.int64)])\n",
    "    @singleton(\"target_encoder\")\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.context_encoder)\n",
    "        set_requires_grad(target_encoder, False)\n",
    "        return target_encoder\n",
    "    \n",
    "#    def reset_moving_average(self):\n",
    "#        del self.target_encoder\n",
    "#        self.target_encoder = None\n",
    "        \n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        context = self.context_encoder(x, context_mask)\n",
    "        context_outputs = self.predictor(context, context_mask, target_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder()\n",
    "            target_outputs = target_encoder(x, target_mask)\n",
    "        \n",
    "        return context_outputs, target_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jepa = Jepa()\n",
    "jepa.to(device)\n",
    "jepa = nn.DataParallel(jepa)\n",
    "jepa.load_state_dict(torch.load(\"/data2/patho-vit_5_23_ccsurv/patient/up/57/gpvit_weight_12_6_i_250.pt\"), strict = False)\n",
    "\n",
    "#jepa.load_state_dict(torch.load(\"/data2/patho-vit_5_23_lung/output_jepa2_96/gpvit_weight_5_28.pt\"), strict = True)\n",
    "\n",
    "# 下臂清零\n",
    "#from collections import OrderedDict\n",
    "#weights = torch.load(\"output_jepa/gpvit_weight_4_13_epoch_16.pt\")\n",
    "\n",
    "#new_dict = OrderedDict()\n",
    "#for k, v in weights.items():\n",
    "#    if \"module.target_encoder\" not in k:\n",
    "#        new_key = k[7:]\n",
    "#        new_dict[new_key] = v\n",
    "\n",
    "#jepa = Jepa()\n",
    "#jepa.to(device)\n",
    "#jepa.load_state_dict(new_dict, strict = False)\n",
    "#jepa = nn.DataParallel(jepa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from collections import OrderedDict\n",
    "#weights = torch.load(\"/data2/patho-vit_5_23_lung/output_jepa2_96/gpvit_weight_5_28.pt\")\n",
    "\n",
    "#new_dict = OrderedDict()\n",
    "#new_dict2 = OrderedDict()\n",
    "#for k, v in weights.items():\n",
    "#    if \"to_patch_embedding.1\" not in k:\n",
    "#        new_key = k[7:]\n",
    "#        new_dict[new_key] = v\n",
    "#    for k2, v2 in new_dict.items():\n",
    "#        if \"to_patch_embedding.2\" not in k2:\n",
    "#            new_key = k2\n",
    "#            new_dict2[new_key] = v2\n",
    "\n",
    "#jepa = Jepa()\n",
    "#jepa.to(device)\n",
    "#jepa.load_state_dict(new_dict2, strict = False)\n",
    "#jepa = nn.DataParallel(jepa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(jepa.parameters(), lr= 1.5e-4, weight_decay = 0.2, betas = [0.9, 0.95])\n",
    "#optimizer = torch.optim.AdamW(mae.parameters(), lr= 3e-4)\n",
    "# 凯明原文，lr的计算方法为：基础lr（1.5e-4）* batch_size / 256。2月15日尝试使用，首次出现了每13轮的反弹，又改回3e-4\n",
    "\n",
    "# 凯明原文，预热迭代40次，是指模型的学习速度从极低的值慢慢涨到指定的lr，理论上可避免训练初期，loss的严重振荡。\n",
    "# 可能的代码如下。本次宫颈癌未用，下次有需要从头训练的瘤种，可以尝试。\n",
    "# from transformers import get_linear_scheduler_with_warmup\n",
    "# total_steps = len(train_loader) * epochs\n",
    "# scheduler = get_linear_scheduler_with_warmup(optimizer, num_warmup_steps = len(train_loader) * 40, \n",
    "#                                              num_training_step = total_steps)\n",
    "\n",
    "# 凯明使用了余弦退火，但没交待这里的T_0和T_mult是如何设置的\n",
    "#scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = 40, T_mult = 1, eta_min= 0.75e-4)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max = 40)\n",
    "#scheduler = StepLR(optimizer, step_size = 1, gamma = 0.7)\n",
    "\n",
    "fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"w\")\n",
    "fconv.write(\"epoch, metric, value\\n\")\n",
    "fconv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Step [250/597] Loss: 0.0000248395\n",
      "Epoch [1/10000], Step [500/597] Loss: 0.0000131179\n",
      "Epoch [1/10000], Loss: 0.0114892907\n",
      "Epoch [2/10000], Step [250/597] Loss: 0.0000116502\n",
      "Epoch [2/10000], Step [500/597] Loss: 0.0000107932\n",
      "Epoch [2/10000], Loss: 0.0000205968\n",
      "Epoch [3/10000], Step [250/597] Loss: 0.0094552403\n",
      "Epoch [3/10000], Step [500/597] Loss: 0.1566981822\n",
      "Epoch [3/10000], Loss: 0.1075044423\n",
      "Epoch [4/10000], Step [250/597] Loss: 0.0114852153\n",
      "Epoch [4/10000], Step [500/597] Loss: 0.0135302274\n",
      "Epoch [4/10000], Loss: 0.0402581133\n",
      "Epoch [5/10000], Step [250/597] Loss: 0.0000421214\n",
      "Epoch [5/10000], Step [500/597] Loss: 0.0000651393\n",
      "Epoch [5/10000], Loss: 0.0072106989\n",
      "Epoch [6/10000], Step [250/597] Loss: 0.0000467023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#m = next(momentum_scheduler)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/adamw.py:137\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/_functional.py:125\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    122\u001b[0m step \u001b[38;5;241m=\u001b[39m state_steps[i]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[1;32m    128\u001b[0m bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 0.7\n",
    "\n",
    "loss_curve = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (images, context_mask, target_mask) in enumerate(train_loader):\n",
    "        #images = images[0][0].to(device)\n",
    "        context_outputs, target_outputs = jepa(images[0], context_mask, target_mask) # 此处有标签，所以用[0]\n",
    "        loss = loss_fn(context_outputs, target_outputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #m = next(momentum_scheduler)\n",
    "            m = 0.9\n",
    "            for param_q, param_k in zip(jepa.module.context_encoder.parameters(), jepa.module.target_encoder.parameters()):\n",
    "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "        \n",
    "        epoch_loss += loss / total_step\n",
    "    \n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.10f}\"\n",
    "                 .format(epoch+1, epochs, i+1, total_step, loss))\n",
    "            \n",
    "            #if loss < best_loss:\n",
    "            #    best_loss = loss\n",
    "            #    torch.save(jepa.state_dict(), \"57/gpvit_weight_12_14_i_{}.pt\".format(i+1))            \n",
    "            \n",
    "            fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"a\")\n",
    "            fconv.write(\"{}, loss, {:.4f}\\n\".format(i+1, loss.item()))\n",
    "            fconv.close()\n",
    "    \n",
    "    #scheduler.step()\n",
    "    loss_curve.append(epoch_loss.cpu().detach())\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "    #if (epoch) % 1 == 0:\n",
    "        print(\"Epoch [{}/{}], Loss: {:.10f}\"\n",
    "             .format(epoch+1, epochs, epoch_loss.item()))\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"a\")\n",
    "        fconv.write(\"{}, loss, {:.10f}\\n\".format(epoch+1, epoch_loss.item()))\n",
    "        fconv.close()\n",
    "    \n",
    "    #if (epoch+1) >= 5:\n",
    "    #if epoch % 1 == 0:\n",
    "    if (epoch+1) % 1 == 0 and epoch_loss < best_loss:\n",
    "        #if epoch % 1 == 0 and loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(jepa.state_dict(), \"57/gpvit_weight_12_23_epoch_{}.pt\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(jepa.state_dict(), \"output_new/gpvit_weight_04_13.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image2 = transform(image)\n",
    "plt.imshow(image2.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_curve = []\n",
    "\n",
    "for epoch in range(4000):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "        #images = images[0][0].to(device)\n",
    "    context_outputs, target_outputs = jepa(image2.unsqueeze(0), context_mask, target_mask) # 此处有标签，所以用[0]\n",
    "    loss = loss_fn(context_outputs, target_outputs)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        #m = next(momentum_scheduler)\n",
    "        m = 0.9\n",
    "        for param_q, param_k in zip(jepa.module.context_encoder.parameters(), jepa.module.target_encoder.parameters()):\n",
    "            param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "        \n",
    "    epoch_loss = loss \n",
    "    \n",
    "    \n",
    "    print(\"Epoch [{}/{}], Loss: {:.10f}\"\n",
    "                .format(epoch+1, 4000, loss))\n",
    "            \n",
    "        #    fconv = open(os.path.join(\"output/gpvit_convergence.csv\"), \"a\")\n",
    "        #    fconv.write(\"{}, loss, {:.4f}\\n\".format(epoch+1, loss.item()))\n",
    "        #    fconv.close()\n",
    "    \n",
    "    #scheduler.step()\n",
    "loss_curve.append(epoch_loss.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "rearrange = Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = 16, p2 = 16)\n",
    "rearrange2 = Rearrange(\"b (h w) (p1 p2 c) -> b c (h p1) (w p2)\", h = 24, p1 = 16, p2 = 16)\n",
    "image_patches = rearrange(image2.unsqueeze(0))\n",
    "image_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_cache = []\n",
    "for target_mask_iter in target_mask:\n",
    "    target_mask_cache.append(target_mask_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_range = torch.arange(1, device = device)[:, None]\n",
    "context_indices = context_mask[0][0]\n",
    "\n",
    "target1_indices = target_mask_cache[0]\n",
    "\n",
    "target2_indices = target_mask_cache[1]\n",
    "\n",
    "target3_indices = target_mask_cache[2]\n",
    "\n",
    "target4_indices = target_mask_cache[3]\n",
    "\n",
    "context_tokens = image_patches[batch_range, context_indices]\n",
    "target1_tokens = image_patches[batch_range, target1_indices]\n",
    "target2_tokens = image_patches[batch_range, target2_indices]\n",
    "target3_tokens = image_patches[batch_range, target3_indices]\n",
    "target4_tokens = image_patches[batch_range, target4_indices]\n",
    "\n",
    "context_patches = torch.zeros(1, 576, 768, device = device)\n",
    "target1_patches = copy.deepcopy(context_patches)\n",
    "target2_patches = copy.deepcopy(context_patches)\n",
    "target3_patches = copy.deepcopy(context_patches)\n",
    "target4_patches = copy.deepcopy(context_patches)\n",
    "\n",
    "context_patches[batch_range, context_indices] = context_tokens\n",
    "target1_patches[batch_range, target1_indices] = target1_tokens\n",
    "target2_patches[batch_range, target2_indices] = target2_tokens\n",
    "target3_patches[batch_range, target3_indices] = target3_tokens\n",
    "target4_patches[batch_range, target4_indices] = target4_tokens\n",
    "\n",
    "context_patches = rearrange2(context_patches)\n",
    "target1_patches = rearrange2(target1_patches)\n",
    "target2_patches = rearrange2(target2_patches)\n",
    "target3_patches = rearrange2(target3_patches)\n",
    "target4_patches = rearrange2(target4_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6)\n",
    "\n",
    "axs[0].imshow(image2.permute(1, 2, 0));\n",
    "axs[0].axis(\"off\");\n",
    "\n",
    "#axs[1].imshow(recon_image.cpu().detach().permute(1, 2, 0));\n",
    "axs[1].imshow(context_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[1].axis(\"off\");\n",
    "        \n",
    "axs[2].imshow(target1_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[2].axis(\"off\");\n",
    "        \n",
    "axs[3].imshow(target2_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[3].axis(\"off\");\n",
    "\n",
    "axs[4].imshow(target3_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[4].axis(\"off\");\n",
    "        \n",
    "axs[5].imshow(target4_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[5].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023",
   "language": "python",
   "name": "2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
