{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "label_csv = None\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "if exists(label_csv):\n",
    "    df_label = pd.read_excel(label_csv, index_col = \"pathid\")\n",
    "pathid = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "path = 'yourpath/low'\n",
    "pathid_orig = [f[:-3] for f in os.listdir(os.path.join(path, \"store_5_96\")) if not f.startswith(\".\")]\n",
    "\n",
    "for i, id in enumerate(pathid_orig):\n",
    "    pathid.append(id)\n",
    "    images.append(os.path.join(path, \"store_5_96\", id+\".db\"))\n",
    "    if exists(label_csv):\n",
    "        labels.append(df_label.loc[id][\"labels\"])\n",
    "    else:\n",
    "        labels.append(1e-6)\n",
    "            \n",
    "\n",
    "c = {\"pathid\": pathid, \"images\": images, \"labels\": labels}\n",
    "df = pd.DataFrame(c)\n",
    "\n",
    "df.to_csv(\"files_label/store_5_96.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "torch.save(df.to_dict(orient='list'), \"files_label/store_5_96.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import copy\n",
    "from functools import wraps, partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from einops import repeat, rearrange \n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"yourpath/low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from multiprocessing import Value\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "import torch\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class MaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size = (96, 96),      \n",
    "        patch_size = 2,             \n",
    "        enc_mask_scale = (0.85, 1.0),\n",
    "        #pred_mask_scale = (0.15, 0.2), \n",
    "        pred_mask_scale = (0.01, 0.01),\n",
    "        aspect_ratio = (0.75, 1.5),   \n",
    "        nenc = 1,                     \n",
    "        npred = 4,                    \n",
    "        min_keep = 4,                 \n",
    "        allow_overlap = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  \n",
    "        self.allow_overlap = allow_overlap \n",
    "        self._itr_counter = Value('i', -1)  \n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        \n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "       \n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        \n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "       \n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            \n",
    "            top = torch.randint(0, self.height - h, (1,))\n",
    "            left = torch.randint(0, self.width - w, (1,))\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            \n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "          \n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        B = len(batch)\n",
    "\n",
    "        collated_batch = default_collate(batch)\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions= None\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_vit.airport3 import layer2dataset\n",
    "\n",
    "train_lib = \"yourpath/low/files_label/store_5_96.db\"\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = layer2dataset(train_lib, transform = transform, subsample = 96)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    collate_fn = MaskCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_mask(x, mask):\n",
    "    all_x = []\n",
    "    for m in mask:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def loss_fn(z, h):\n",
    "    loss = F.smooth_l1_loss(z, h)\n",
    "    loss = loss.sum(dim = -1).mean()\n",
    "    return loss\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 12, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_encoder(nn.Module):\n",
    "    def __init__(self, *, image_size = 96, patch_size = 2, channels = 768, emb_dropout = 0., \n",
    "                 dim = 768 * 2, depth = 35, heads = 12 * 2, dim_head = 64, mlp_dim = 768 * 2 * 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        device = img.device\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = apply_mask(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, *, num_patches = 48 * 48, encoder_dim = 768 * 2, \n",
    "                 decoder_dim = 768 * 2, decoder_depth = 8, decoder_heads = 12 * 2, decoder_dim_head = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.black_token = nn.Parameter(torch.randn(1, 1, decoder_dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "\n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        device = x.device\n",
    "        batch, num_white, _ = x.shape\n",
    "        \n",
    "        white_tokens = self.enc_to_dec(x)\n",
    "        white_tokens += self.decoder_pos_emb(context_mask[0])\n",
    "        \n",
    "        black_tokens1 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[0].size(-1))\n",
    "        black_tokens1 = black_tokens1 + self.decoder_pos_emb(target_mask[0])\n",
    "        \n",
    "        black_tokens2 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[1].size(-1))\n",
    "        black_tokens2 = black_tokens2 + self.decoder_pos_emb(target_mask[1])\n",
    "        \n",
    "        black_tokens3 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[2].size(-1))\n",
    "        black_tokens3 = black_tokens3 + self.decoder_pos_emb(target_mask[2])\n",
    "        \n",
    "        black_tokens4 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[3].size(-1))\n",
    "        black_tokens4 = black_tokens4 + self.decoder_pos_emb(target_mask[3])\n",
    "        \n",
    "        decoder_tokens = torch.zeros(batch, self.num_patches, self.decoder_dim, device=device)\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        decoder_tokens[batch_range, context_mask[0]] = white_tokens\n",
    "        decoder_tokens[batch_range, target_mask[0]] = black_tokens1\n",
    "        decoder_tokens[batch_range, target_mask[1]] = black_tokens2\n",
    "        decoder_tokens[batch_range, target_mask[2]] = black_tokens3\n",
    "        decoder_tokens[batch_range, target_mask[3]] = black_tokens4\n",
    "        \n",
    "        decoder_tokens = self.decoder(decoder_tokens)\n",
    "        \n",
    "        context = apply_mask(decoder_tokens, target_mask)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jepa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_encoder = Context_encoder()\n",
    "        self.predictor = Predictor()\n",
    "        self.target_encoder = None\n",
    "        \n",
    "        device = get_module_device(Context_encoder())\n",
    "        self.to(device)\n",
    "        \n",
    "        self.forward(torch.randn(2, 768, 96, 96, device = device), torch.ones(2, 48 * 48, dtype  = torch.int64), [torch.ones(2, 48 * 48, dtype  = torch.int64), \n",
    "                                                                                                             torch.ones(2, 48 * 48, dtype  = torch.int64), \n",
    "                                                                                                              torch.ones(2, 48 * 48, dtype  = torch.int64),                                     \n",
    "                                                                                                              torch.ones(2, 48 * 48, dtype  = torch.int64)])\n",
    "    @singleton(\"target_encoder\")\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.context_encoder)\n",
    "        set_requires_grad(target_encoder, False)\n",
    "        return target_encoder\n",
    "\n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        context = self.context_encoder(x, context_mask)\n",
    "        context_outputs = self.predictor(context, context_mask, target_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder()\n",
    "            target_outputs = target_encoder(x, target_mask)\n",
    "        \n",
    "        return context_outputs, target_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jepa = Jepa()\n",
    "jepa.to(device)\n",
    "jepa = nn.DataParallel(jepa)\n",
    "jepa.load_state_dict(torch.load(\"yourpath/up/57/**.pt\"), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(jepa.parameters(), lr= 1.5e-4, weight_decay = 0.2, betas = [0.9, 0.95])\n",
    "fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"w\")\n",
    "fconv.write(\"epoch, metric, value\\n\")\n",
    "fconv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_loss = 0.7\n",
    "\n",
    "loss_curve = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (images, context_mask, target_mask) in enumerate(train_loader):\n",
    "     \n",
    "        context_outputs, target_outputs = jepa(images[0], context_mask, target_mask) \n",
    "        loss = loss_fn(context_outputs, target_outputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            m = 0.9\n",
    "            for param_q, param_k in zip(jepa.module.context_encoder.parameters(), jepa.module.target_encoder.parameters()):\n",
    "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "        \n",
    "        epoch_loss += loss / total_step\n",
    "    \n",
    "        if (i + 1) % 250 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.10f}\"\n",
    "                 .format(epoch+1, epochs, i+1, total_step, loss))\n",
    "     \n",
    "            fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"a\")\n",
    "            fconv.write(\"{}, loss, {:.4f}\\n\".format(i+1, loss.item()))\n",
    "            fconv.close()\n",
    "    \n",
    "   \n",
    "    loss_curve.append(epoch_loss.cpu().detach())\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "  \n",
    "        print(\"Epoch [{}/{}], Loss: {:.10f}\"\n",
    "             .format(epoch+1, epochs, epoch_loss.item()))\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        fconv = open(os.path.join(\"57/gpvit_convergence.csv\"), \"a\")\n",
    "        fconv.write(\"{}, loss, {:.10f}\\n\".format(epoch+1, epoch_loss.item()))\n",
    "        fconv.close()\n",
    "    \n",
    "   \n",
    "    if (epoch+1) % 1 == 0 and epoch_loss < best_loss:\n",
    "       \n",
    "        best_loss = epoch_loss\n",
    "        torch.save(jepa.state_dict(), \"57/gpvit_weight_12_23_epoch_{}.pt\".format(epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023",
   "language": "python",
   "name": "2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
