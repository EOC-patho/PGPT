{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存默一贴摁卡，图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import copy\n",
    "from functools import wraps, partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from einops import repeat, rearrange \n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"/data2/patho-vit_5_23_ccsurv/up\"\n",
    "# 此处patho_vit后的_5_23为包的版本号\n",
    "# 若此jupyternotebook运行中kernel挂掉，重启后仅需运行此一代码块，然后跳到需要运行的代码块即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部门：机场1——全面训练海绵宝宝\n",
    "batch_size = 260\n",
    "# 原始文献为50，若报错CUDA OUT OF MEMORY,则减少batch_size个数。此外，需关闭kernel重启，只重启机场1代码即可。\n",
    "num_workers = 48\n",
    "# 原始文献此处为4\n",
    "\n",
    "epochs = 10000\n",
    "# 原始文献中默认值为50\n",
    "#test_every = 1\n",
    "# 原始文献中默认值为10\n",
    "\n",
    "#n = 0\n",
    "# n在0到 400/25 = 16之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from multiprocessing import Value\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "import torch\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class MaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size = (384, 384),\n",
    "        patch_size = 16,\n",
    "        enc_mask_scale = (0.85, 1.0),\n",
    "        pred_mask_scale = (0.15, 0.2),\n",
    "        aspect_ratio = (0.75, 1.5),\n",
    "        nenc = 1,\n",
    "        npred = 4,\n",
    "        min_keep = 4,\n",
    "        allow_overlap = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  # minimum number of patches to keep\n",
    "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        # -- Sample block scale\n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "        # -- Sample block aspect-ratio\n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        # -- Compute block height and width (given scale and aspect-ratio)\n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "        # --\n",
    "        # -- Loop to sample masks until we find a valid one\n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            # -- Sample block top-left corner\n",
    "            top = torch.randint(0, self.height - h, (1,))\n",
    "            left = torch.randint(0, self.width - w, (1,))\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            # -- Constrain mask to a set of acceptable regions\n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "            # -- If mask too small try again\n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        B = len(batch)\n",
    "\n",
    "        collated_batch = default_collate(batch)\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions= None\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize([384, 384]),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "#train_lib = \"files/store_5_train_{}.db\".format((n+1) * 25)\n",
    "train_lib = \"/data2/patho-vit_5_23_ccsurv/SZL_exva/files/store_4.db\"\n",
    "\n",
    "#valid_lib = \"files/store_3_valid.db\"\n",
    "\n",
    "from patho_vit.airport1 import pathovitdataset\n",
    "\n",
    "train_dataset = pathovitdataset(libraryfile = train_lib, transform = transform)\n",
    "\n",
    "#valid_dataset = pathovitdataset(libraryfile = train_lib, transform = transform)\n",
    "\n",
    "#x_test = []\n",
    "#y_test = []\n",
    "#for i in range(4):\n",
    "#    x = valid_dataset[i][0]\n",
    "#    x_test.append(x)\n",
    "#    y = valid_dataset[i][1]\n",
    "#    y_test.append(y)\n",
    "#test_dataset = [(x, y) for x, y in zip(x_test, y_test)] #包装为数据对\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    collate_fn = MaskCollator()\n",
    ")\n",
    "#valid_loader = torch.utils.data.DataLoader(\n",
    "#    valid_dataset,\n",
    "#    batch_size = batch_size, shuffle = False,\n",
    "#    num_workers = num_workers\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(x, mask):\n",
    "    all_x = []\n",
    "    for m in mask:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "def set_requires_grad(model, val):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = val\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def loss_fn(z, h):\n",
    "    loss = F.smooth_l1_loss(z, h)\n",
    "    loss = loss.sum(dim = -1).mean()\n",
    "    return loss\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 12, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Context_encoder(nn.Module):\n",
    "    def __init__(self, *, image_size = 384, patch_size = 16, channels = 3, emb_dropout = 0., \n",
    "                 dim = 768, depth = 12, heads = 12, dim_head = 64, mlp_dim = 768 * 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        device = img.device\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = apply_mask(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, *, num_patches = 576, encoder_dim = 768, \n",
    "                 decoder_dim = 768, decoder_depth = 8, decoder_heads = 8, decoder_dim_head = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.black_token = nn.Parameter(torch.randn(1, 1, decoder_dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "\n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        device = x.device\n",
    "        batch, num_white, _ = x.shape\n",
    "        \n",
    "        white_tokens = self.enc_to_dec(x)\n",
    "        white_tokens += self.decoder_pos_emb(context_mask[0])\n",
    "        \n",
    "        black_tokens1 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[0].size(-1))\n",
    "        black_tokens1 = black_tokens1 + self.decoder_pos_emb(target_mask[0])\n",
    "        \n",
    "        black_tokens2 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[1].size(-1))\n",
    "        black_tokens2 = black_tokens2 + self.decoder_pos_emb(target_mask[1])\n",
    "        \n",
    "        black_tokens3 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[2].size(-1))\n",
    "        black_tokens3 = black_tokens3 + self.decoder_pos_emb(target_mask[2])\n",
    "        \n",
    "        black_tokens4 = repeat(self.black_token, '1 1 d -> b n d', b = batch, n = target_mask[3].size(-1))\n",
    "        black_tokens4 = black_tokens4 + self.decoder_pos_emb(target_mask[3])\n",
    "        \n",
    "        decoder_tokens = torch.zeros(batch, self.num_patches, self.decoder_dim, device=device)\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        decoder_tokens[batch_range, context_mask[0]] = white_tokens\n",
    "        decoder_tokens[batch_range, target_mask[0]] = black_tokens1\n",
    "        decoder_tokens[batch_range, target_mask[1]] = black_tokens2\n",
    "        decoder_tokens[batch_range, target_mask[2]] = black_tokens3\n",
    "        decoder_tokens[batch_range, target_mask[3]] = black_tokens4\n",
    "        \n",
    "        decoder_tokens = self.decoder(decoder_tokens)\n",
    "        \n",
    "        context = apply_mask(decoder_tokens, target_mask)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jepa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_encoder = Context_encoder()\n",
    "        self.predictor = Predictor()\n",
    "        #self.target_encoder = Target_encoder()\n",
    "        self.target_encoder = None\n",
    "        \n",
    "        device = get_module_device(Context_encoder())\n",
    "        self.to(device)\n",
    "        \n",
    "        self.forward(torch.randn(2, 3, 384, 384, device = device), torch.ones(2, 576, dtype  = torch.int64), [torch.ones(2, 576, dtype  = torch.int64), \n",
    "                                                                                                             torch.ones(2, 576, dtype  = torch.int64), \n",
    "                                                                                                              torch.ones(2, 576, dtype  = torch.int64),                                     \n",
    "                                                                                                              torch.ones(2, 576, dtype  = torch.int64)])\n",
    "    @singleton(\"target_encoder\")\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.context_encoder)\n",
    "        set_requires_grad(target_encoder, False)\n",
    "        return target_encoder\n",
    "    \n",
    "#    def reset_moving_average(self):\n",
    "#        del self.target_encoder\n",
    "#        self.target_encoder = None\n",
    "        \n",
    "    def forward(self, x, context_mask, target_mask):\n",
    "        context = self.context_encoder(x, context_mask)\n",
    "        context_outputs = self.predictor(context, context_mask, target_mask)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder()\n",
    "            target_outputs = target_encoder(x, target_mask)\n",
    "        \n",
    "        return context_outputs, target_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下臂清零\n",
    "#from collections import OrderedDict\n",
    "#weights = torch.load(\"/data2/patho-vit_5_23_lung/output_jepa/gpvit_weight_5_5_epoch_48.pt\")\n",
    "\n",
    "#new_dict = OrderedDict()\n",
    "#for k, v in weights.items():\n",
    "#    if \"module.target_encoder\" not in k:\n",
    "#        new_key = k[7:]\n",
    "#        new_dict[new_key] = v\n",
    "\n",
    "#jepa = Jepa()\n",
    "#jepa.to(device)\n",
    "#jepa.load_state_dict(new_dict, strict = False)\n",
    "#jepa = nn.DataParallel(jepa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "#weights = torch.load(\"output_jepa/gpvit_weight_4_13_epoch_16.pt\")\n",
    "weights = torch.load(\"/data2/patho-vit_5_23_ccim/up/13/11.20waibu/gpvit_weight_11_20_i_600.pt\")\n",
    "#new_dict = OrderedDict()\n",
    "#for k, v in weights.items():\n",
    "#    if \"module.target_encoder\" not in k:\n",
    "#        new_key = k[7:]\n",
    "#        new_dict[new_key] = v\n",
    "\n",
    "jepa = Jepa()\n",
    "jepa.to(device)\n",
    "#jepa.load_state_dict(new_dict, strict = True)\n",
    "jepa = nn.DataParallel(jepa)\n",
    "jepa.load_state_dict(weights, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(jepa.parameters(), lr= 1.5e-4, weight_decay = 0.2, betas = [0.9, 0.95])\n",
    "#optimizer = torch.optim.AdamW(mae.parameters(), lr= 3e-4)\n",
    "# 凯明原文，lr的计算方法为：基础lr（1.5e-4）* batch_size / 256。2月15日尝试使用，首次出现了每13轮的反弹，又改回3e-4\n",
    "\n",
    "# 凯明原文，预热迭代40次，是指模型的学习速度从极低的值慢慢涨到指定的lr，理论上可避免训练初期，loss的严重振荡。\n",
    "# 可能的代码如下。本次宫颈癌未用，下次有需要从头训练的瘤种，可以尝试。\n",
    "# from transformers import get_linear_scheduler_with_warmup\n",
    "# total_steps = len(train_loader) * epochs\n",
    "# scheduler = get_linear_scheduler_with_warmup(optimizer, num_warmup_steps = len(train_loader) * 40, \n",
    "#                                              num_training_step = total_steps)\n",
    "\n",
    "# 凯明使用了余弦退火，但没交待这里的T_0和T_mult是如何设置的\n",
    "#scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = 40, T_mult = 1, eta_min= 0.75e-4)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max = 40)\n",
    "#scheduler = StepLR(optimizer, step_size = 1, gamma = 0.7)\n",
    "\n",
    "fconv = open(os.path.join(\"13/12.4/gpvit_convergence.csv\"), \"w\")\n",
    "fconv.write(\"epoch, metric, value\\n\")\n",
    "fconv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 10.8348598480\n",
      "Epoch [2/10000], Loss: 10.8564672470\n",
      "Epoch [3/10000], Loss: 9.9641628265\n",
      "Epoch [4/10000], Loss: 9.8310670853\n",
      "Epoch [5/10000], Loss: 11.4308090210\n",
      "Epoch [6/10000], Loss: 11.1752147675\n",
      "Epoch [7/10000], Loss: 10.2119522095\n",
      "Epoch [8/10000], Loss: 10.1589555740\n",
      "Epoch [9/10000], Loss: 10.3110685349\n",
      "Epoch [10/10000], Loss: 10.4685993195\n",
      "Epoch [11/10000], Loss: 9.6523704529\n",
      "Epoch [12/10000], Loss: 9.8823804855\n",
      "Epoch [13/10000], Loss: 9.9025526047\n",
      "Epoch [14/10000], Loss: 9.7417573929\n",
      "Epoch [15/10000], Loss: 9.3705539703\n",
      "Epoch [16/10000], Loss: 10.1730194092\n",
      "Epoch [17/10000], Loss: 9.2606277466\n",
      "Epoch [18/10000], Loss: 8.8095960617\n",
      "Epoch [19/10000], Loss: 9.4374237061\n",
      "Epoch [20/10000], Loss: 9.3924455643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10e5\u001b[39m   \u001b[38;5;66;03m#6.20第1次\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#m = next(momentum_scheduler)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 10e5\n",
    "best_epoch_loss = 10e5\n",
    "loss_curve = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (images, context_mask, target_mask) in enumerate(train_loader):\n",
    "        #images = images[0][0].to(device)\n",
    "        context_outputs, target_outputs = jepa(images[0], context_mask, target_mask) # 此处有标签，所以用[0]\n",
    "        loss = loss_fn(context_outputs, target_outputs)\n",
    "        loss *= 10e5   #6.20第1次\n",
    "           \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #m = next(momentum_scheduler)\n",
    "            m = 0.9\n",
    "            for param_q, param_k in zip(jepa.module.context_encoder.parameters(), jepa.module.target_encoder.parameters()):\n",
    "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "        \n",
    "        epoch_loss += loss / total_step\n",
    "    \n",
    "        \n",
    "        #if (i + 1) % 15000 == 0:\n",
    "        #if (i + 1) % 1 == 0:\n",
    "        #    print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.10f}\"\n",
    "       #          .format(epoch+1, epochs, i+1, total_step, loss))\n",
    "            \n",
    "        if (i + 1) % 200 == 0:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save(jepa.state_dict(), \"13/12.4/gpvit_weight_12_4_i_{}.pt\".format(i+1))\n",
    "            fconv = open(os.path.join(\"13/12.4/gpvit_convergence.csv\"), \"a\")\n",
    "            fconv.write(\"{}, loss, {:.10f}\\n\".format(i+1, loss.item()))\n",
    "            fconv.close()\n",
    "    \n",
    "    #scheduler.step()\n",
    "    loss_curve.append(epoch_loss.cpu().detach())\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "    #if (epoch) % 1 == 0:\n",
    "        if  epoch_loss < best_epoch_loss:\n",
    "            best_epoch_loss = epoch_loss\n",
    "            torch.save(jepa.state_dict(), \"13/12.4/gpvit_weight_12_4_epoch_{}.pt\".format(epoch+1))\n",
    "        \n",
    "        print(\"Epoch [{}/{}], Loss: {:.10f}\"\n",
    "             .format(epoch+1, epochs, epoch_loss.item()))\n",
    "        \n",
    "        fconv = open(os.path.join(\"13/12.4/gpvit_convergence.csv\"), \"a\")\n",
    "        fconv.write(\"{}, loss, {:.10f}\\n\".format(epoch+1, epoch_loss.item()))\n",
    "        fconv.close()\n",
    "    \n",
    "    #if (epoch+1) >= 5:\n",
    "    #if epoch % 1 == 0:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(jepa.state_dict(), \"13/gpvit_weight_7_29_epoch_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机场2，为每个补丁抽出特征，为整张切片制作特征图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"/data2/patho-vit_5_23_ccsurv\"\n",
    "# 此处patho_vit后的_5_23为包的版本号\n",
    "# 若此jupyternotebook运行中kernel挂掉，重启后仅需运行此一代码块，然后跳到需要运行的代码块即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_vit.vit_luci2 import ViT as ViT\n",
    "from patho_vit.airport2 import train_features_extractor_gpvit2 as Extractor\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT(\n",
    "    image_size = 384,\n",
    "    patch_size = 16,\n",
    "    dim = 768,\n",
    "    depth = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 768 * 4,\n",
    "    num_classes = 1000\n",
    ")\n",
    "output, feature = vit(torch.randn(2, 3, 384, 384))\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入定制vit，加载目标编码器的权重\n",
    "\n",
    "#weights = torch.load(\"output_jepa/gpvit_weight_4_22_epoch_1.pt\")\n",
    "weights = torch.load(\"/data2/patho-vit_5_23_ccsurv/up/13/12.4/gpvit_weight_12_4_epoch_18.pt\", map_location=torch.device('cpu'))\n",
    "path = \"/data2/patho-vit_5_23_ccsurv/up\"\n",
    "new_dict = OrderedDict()\n",
    "for k, v in weights.items():\n",
    "    if \"module.target_encoder\" in k:\n",
    "        new_key = k[22:]\n",
    "        new_dict[new_key] = v\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 384,\n",
    "    patch_size = 16,\n",
    "    dim = 768,\n",
    "    depth = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 768 * 4,\n",
    "    num_classes = 1000\n",
    ")\n",
    "vit.to(device)\n",
    "vit.load_state_dict(new_dict, strict = False)\n",
    "vit = nn.DataParallel(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 一个文件夹一个文件夹的抽特征，并重构（384，384，768）的图像\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([384, 384]),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "#train_lib = \"files/store_5_train_{}.db\".format((n+1) * 25)\n",
    "train_lib_0 = torch.load(\"/data2/patho-vit_5_23_ccim/up/files_label/store_4_qlyy.db\")\n",
    "#train_lib_1 = torch.load(\"/data2/patho-vit_5_23_ccim/up/files_label/store_4_PD+WUQIAN.db\")\n",
    "#train_lib_2 = torch.load(\"/data2/patho-vit_5_23_ccim/up/files_label/store_4_im.db\")\n",
    "#train_lib_3 = torch.load(\"/data2/patho-vit_5_23_ccim/up/files_label/store_4_qlyywaibu.db\")\n",
    "extractor0 = Extractor(path = path, train_lib = train_lib_0, transform = transform, batch_size = 200, model = vit, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor1 = Extractor(path = path, train_lib = train_lib_1, transform = transform, batch_size = 200, model = vit, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor2 = Extractor(path = path, train_lib = train_lib_2, transform = transform, batch_size = 200, model = vit, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor3 = Extractor(path = path, train_lib = train_lib_3, transform = transform, batch_size = 200, model = vit, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"cat.jpg\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image2 = transform(image)\n",
    "plt.imshow(image2.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_curve = []\n",
    "\n",
    "for epoch in range(4000):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "        #images = images[0][0].to(device)\n",
    "    context_outputs, target_outputs = jepa(image2.unsqueeze(0), context_mask, target_mask) # 此处有标签，所以用[0]\n",
    "    loss = loss_fn(context_outputs, target_outputs)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        #m = next(momentum_scheduler)\n",
    "        m = 0.9\n",
    "        for param_q, param_k in zip(jepa.module.context_encoder.parameters(), jepa.module.target_encoder.parameters()):\n",
    "            param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "        \n",
    "    epoch_loss = loss \n",
    "    \n",
    "    \n",
    "    print(\"Epoch [{}/{}], Loss: {:.10f}\"\n",
    "                .format(epoch+1, 4000, loss))\n",
    "            \n",
    "        #    fconv = open(os.path.join(\"output/gpvit_convergence.csv\"), \"a\")\n",
    "        #    fconv.write(\"{}, loss, {:.4f}\\n\".format(epoch+1, loss.item()))\n",
    "        #    fconv.close()\n",
    "    \n",
    "    #scheduler.step()\n",
    "loss_curve.append(epoch_loss.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "rearrange = Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = 16, p2 = 16)\n",
    "rearrange2 = Rearrange(\"b (h w) (p1 p2 c) -> b c (h p1) (w p2)\", h = 24, p1 = 16, p2 = 16)\n",
    "image_patches = rearrange(image2.unsqueeze(0))\n",
    "image_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask_cache = []\n",
    "for target_mask_iter in target_mask:\n",
    "    target_mask_cache.append(target_mask_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_range = torch.arange(1, device = device)[:, None]\n",
    "context_indices = context_mask[0][0]\n",
    "\n",
    "target1_indices = target_mask_cache[0]\n",
    "\n",
    "target2_indices = target_mask_cache[1]\n",
    "\n",
    "target3_indices = target_mask_cache[2]\n",
    "\n",
    "target4_indices = target_mask_cache[3]\n",
    "\n",
    "context_tokens = image_patches[batch_range, context_indices]\n",
    "target1_tokens = image_patches[batch_range, target1_indices]\n",
    "target2_tokens = image_patches[batch_range, target2_indices]\n",
    "target3_tokens = image_patches[batch_range, target3_indices]\n",
    "target4_tokens = image_patches[batch_range, target4_indices]\n",
    "\n",
    "context_patches = torch.zeros(1, 576, 768, device = device)\n",
    "target1_patches = copy.deepcopy(context_patches)\n",
    "target2_patches = copy.deepcopy(context_patches)\n",
    "target3_patches = copy.deepcopy(context_patches)\n",
    "target4_patches = copy.deepcopy(context_patches)\n",
    "\n",
    "context_patches[batch_range, context_indices] = context_tokens\n",
    "target1_patches[batch_range, target1_indices] = target1_tokens\n",
    "target2_patches[batch_range, target2_indices] = target2_tokens\n",
    "target3_patches[batch_range, target3_indices] = target3_tokens\n",
    "target4_patches[batch_range, target4_indices] = target4_tokens\n",
    "\n",
    "context_patches = rearrange2(context_patches)\n",
    "target1_patches = rearrange2(target1_patches)\n",
    "target2_patches = rearrange2(target2_patches)\n",
    "target3_patches = rearrange2(target3_patches)\n",
    "target4_patches = rearrange2(target4_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6)\n",
    "\n",
    "axs[0].imshow(image2.permute(1, 2, 0));\n",
    "axs[0].axis(\"off\");\n",
    "\n",
    "#axs[1].imshow(recon_image.cpu().detach().permute(1, 2, 0));\n",
    "axs[1].imshow(context_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[1].axis(\"off\");\n",
    "        \n",
    "axs[2].imshow(target1_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[2].axis(\"off\");\n",
    "        \n",
    "axs[3].imshow(target2_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[3].axis(\"off\");\n",
    "\n",
    "axs[4].imshow(target3_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[4].axis(\"off\");\n",
    "        \n",
    "axs[5].imshow(target4_patches.squeeze(0).permute(1, 2, 0));\n",
    "axs[5].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023",
   "language": "python",
   "name": "2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
