{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e5780-5ad7-4210-8d13-1000559215b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/data2/ccimmu/省肿瘤'\n",
    "label_csv = None\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "pathid_orig = [f[:-6] for f in os.listdir(os.path.join(path, \"store_4\")) if not f.startswith(\".\")]\n",
    "    # 【:-6】的原因：pyvips切割wsi后，生成的文件自动加上了后缀_files,正好是6个字节\n",
    "    #pathid_orig = [f[:-6] for f in os.listdir(os.path.join(path, \"pseudo_store_3\")) if not f.startswith(\".\")]\n",
    "    \n",
    "if exists(label_csv):\n",
    "    df_label = pd.read_csv(label_csv, index_col = \"pathid\")\n",
    "pathid = []\n",
    "images = []\n",
    "labels = []\n",
    "for i, id in enumerate(pathid_orig):\n",
    "    if len(glob.glob(os.path.join(path, \"store_4\", str(id) + \"_files\",\"0\", \"*.jpeg\"))) != 0:\n",
    "        pathid.append(id)\n",
    "        # 若生成的csv中的images为空缺，需检查图片的格式是否正确。例如，需要区分.jpeg和.jpg\n",
    "        #重要提示：注意最后的文件后缀。images.append(glob.glob(os.path.join(path, \"store_3\", str(id) + \"_files\",\"0\", \"*.jpeg\")))\n",
    "        images.append(glob.glob(os.path.join(path, \"store_4\", str(id) + \"_files\",\"0\", \"*.jpeg\")))\n",
    "        if exists(label_csv):\n",
    "            labels.append(df_label.loc[id][\"labels\"])\n",
    "        else:\n",
    "            labels.append(1e-6)\n",
    "    else:\n",
    "        pathid_orig_2 = [f for f in os.listdir(os.path.join(path, \"store_4\", str(id) + \"_files\")) if not f.startswith(\".\")]\n",
    "        for j, id_2 in enumerate(pathid_orig_2):\n",
    "            pathid.append(id_2)\n",
    "            images.append(glob.glob(os.path.join(path, \"store_4\", str(id) + \"_files\", str(id_2), \"0\", \"*.jpeg\")))\n",
    "            if exists(label_csv):\n",
    "                labels.append(df_label.loc[id][\"labels\"])\n",
    "            else:\n",
    "                labels.append(1e-6)\n",
    "c = {\"pathid\": pathid, \"images\": images, \"labels\": labels}\n",
    "#df_temp = pd.DataFrame(c)\n",
    "#df_temp.to_csv(\"files_label/store_4_temp.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "#torch.save(df.to_dict(orient='list'), \"files_label/store_4_temp.db\")\n",
    "#df = pd.read_csv(\"files_label/store_4_temp.csv\", index_col = \"pathid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb65b7dd-7506-4bc6-acb0-68acfe7f0dcc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_csv = \"/data2/patho-vit_5_23_ccim/low/files_label/store_4_cc.csv\"\n",
    "if exists(label_csv):\n",
    "    df_label = pd.read_csv(label_csv)\n",
    "\n",
    "index_train, index_valid = train_test_split(range(len(df_label)), test_size = 0.2)\n",
    "df_train = df_label.loc[index_train].reset_index(drop = True)\n",
    "df_valid = df_label.loc[index_valid].reset_index(drop = True)\n",
    "\n",
    "pathid = []\n",
    "images = []\n",
    "labels = []\n",
    "for i, id in enumerate(df_train.pathid):\n",
    "    pathid.append(id)\n",
    "    #images.append(df.loc[id][\"images\"])\n",
    "    index = c[\"pathid\"].index(id)\n",
    "    images.append(c[\"images\"][index])\n",
    "    labels.append(df_train.loc[i].labels)\n",
    "c2 = {\"pathid\": pathid, \"images\": images, \"labels\": labels}\n",
    "df_train = pd.DataFrame(c2) \n",
    "\n",
    "pathid = []\n",
    "images = []\n",
    "labels = []\n",
    "for i, id in enumerate(df_valid.pathid):\n",
    "    pathid.append(id)\n",
    "    #images.append(df.loc[id][\"images\"])\n",
    "    index = c[\"pathid\"].index(id)\n",
    "    images.append(c[\"images\"][index])\n",
    "    labels.append(df_valid.loc[i].labels)\n",
    "c3 = {\"pathid\": pathid, \"images\": images, \"labels\": labels}\n",
    "df_valid = pd.DataFrame(c3) \n",
    "\n",
    "df_train.to_csv(\"files_label/store_4_train_cc.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "torch.save(df_train.to_dict(orient='list'), \"files_label/store_4_train_cc.db\")\n",
    "\n",
    "df_valid.to_csv(\"files_label/store_4_valid_cc.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "torch.save(df_valid.to_dict(orient='list'), \"files_label/store_4_valid_cc.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c7f22-5b1d-4949-b190-f53ba2b293cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4538c45a-c7c3-47c6-9aa8-62c691ec6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"/data2/patho-vit_5_23_ccsurv\"\n",
    "# 此处patho_vit后的_5_23为包的版本号\n",
    "# 若此jupyternotebook运行中kernel挂掉，重启后仅需运行此一代码块，然后跳到需要运行的代码块即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ef25c5-1fb8-473f-987f-41b1f299bf34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 22\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419edf77-a0e1-4b2b-8712-35ded98e9a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f796cc8-ea1e-409f-892d-a1f4c642c8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24432c87-1485-42fb-8d91-4f2e756a0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二部门：机场1——全面训练海绵宝宝\n",
    "batch_size = 400\n",
    "# 取决于gpu数量，原始文献为50，若报错CUDA OUT OF MEMORY,则减少batch_size个数。此外，需关闭kernel重启，只重启机场1代码即可。\n",
    "num_workers = 60\n",
    "# 取决于cpu数量，原始文献此处为4，可从0逐渐增加\n",
    "\n",
    "epochs = 10000\n",
    "# 原始文献中默认值为50\n",
    "test_every = 1\n",
    "# 多少次训练后，进行一次验证。原始文献中默认值为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f591a8f-0d91-4a79-b9b5-3cf9abfd2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "#train_lib_0 = \"/data2/patho-vit_5_23_ccim/low/files_label/store_4_lung_train.db\"\n",
    "train_lib_1 = \"/data2/patho-vit_5_23_ccsurv/files_label/store_4_train_ccsurv.db\"\n",
    "valid_lib = \"/data2/patho-vit_5_23_ccsurv/files_label/store_4_valid_ccsurv.db\"\n",
    "valid_lib_1 = \"/data2/patho-vit_5_23_ccsurv/SZL_exva/files/store_4.db\"\n",
    "\n",
    "from patho_vit.airport1 import pathovitdataset\n",
    "\n",
    "#train_dataset_0 = pathovitdataset(train_lib_0, transform)\n",
    "#train_loader_0 = torch.utils.data.DataLoader(\n",
    "#    train_dataset_0,\n",
    "#    batch_size = batch_size, shuffle = True,\n",
    "#    num_workers = num_workers)\n",
    "\n",
    "train_dataset_1 = pathovitdataset(train_lib_1, transform)\n",
    "train_loader_1 = torch.utils.data.DataLoader(\n",
    "    train_dataset_1,\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "valid_dataset = pathovitdataset(valid_lib, transform)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = batch_size, shuffle = False,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n",
    "valid_dataset_1 = pathovitdataset(valid_lib_1, transform)\n",
    "valid_loader_1 = torch.utils.data.DataLoader(\n",
    "    valid_dataset_1,\n",
    "    batch_size = batch_size, shuffle = False,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b94a8c8-7c38-4efd-9713-511665957b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from patho_vit.vit_luci import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e6aff6-5af3-42f4-b40f-faf9585dbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    image_size = 384,\n",
    "    patch_size = 16, \n",
    "    channels = 3,\n",
    "    dim = 768,\n",
    "    depth = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 768 * 4,\n",
    "    num_classes = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25029050-1244-4028-b8c2-2c81a6bff098",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入定制vit，加载目标编码器的权重\n",
    "#weights = torch.load(\"/data2/patho-vit_5_23_oc/up/13/gpvit_weight_6_24_epoch_1.pt\")\n",
    "\n",
    "weights = torch.load(\"/data2/patho-vit_5_23_ccsurv/up/13/12.4/gpvit_weight_12_4_epoch_18.pt\")\n",
    "\n",
    "new_dict = OrderedDict()\n",
    "for k, v in weights.items():\n",
    "    if \"module.target_encoder\" in k:\n",
    "        new_key = k[22:]\n",
    "        new_dict[new_key] = v\n",
    "\n",
    "vit.to(device)\n",
    "vit.load_state_dict(new_dict, strict = False)\n",
    "vit = nn.DataParallel(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd8746d-37ca-4f0c-8f40-d46b3360b00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入定制vit，加载目标编码器的权重\n",
    "#weights = torch.load(\"/data2/patho-vit_5_23_oc/up/13/gpvit_weight_6_24_epoch_1.pt\")\n",
    "\n",
    "weights = torch.load(\"/data2/patho-vit_5_23_ccsurv/low/4/12.7/checkpoint_ccsurv_10.pth\")\n",
    "\n",
    "##new_dict = OrderedDict()\n",
    "#for k, v in weights.items():\n",
    "#    if \"module.target_encoder\" in k:\n",
    "#        new_key = k[22:]\n",
    "#        new_dict[new_key] = v\n",
    "\n",
    "vit.to(device)\n",
    "vit = nn.DataParallel(vit)\n",
    "vit.load_state_dict(weights, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131368a6-74cc-4561-8373-882efbf08bcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Step [2461/11392] Loss: 0.3547721803\n",
      "Epoch [1/10000], Step [4922/11392] Loss: 0.4001185298\n",
      "Epoch [1/10000], Step [7383/11392] Loss: 0.3311749101\n",
      "Epoch [1/10000], Step [9844/11392] Loss: 0.3454498947\n",
      "Epoch [1/10000], Loss_cc: 0.3716, Acc_cc: 0.8080\n",
      "Epoch : 1 - val_loss: 0.7607, val_acc: 0.6671 - ba: 0.5111\n",
      "\n",
      "Epoch : 1 - test_acc: 0.7985 - ba_test: 0.5203\n",
      "\n",
      "Epoch [2/10000], Step [2461/11392] Loss: 0.3131391406\n",
      "Epoch [2/10000], Step [4922/11392] Loss: 0.4424652755\n",
      "Epoch [2/10000], Step [7383/11392] Loss: 0.3830794394\n",
      "Epoch [2/10000], Step [9844/11392] Loss: 0.3443254232\n",
      "Epoch [2/10000], Loss_cc: 0.3600, Acc_cc: 0.8152\n",
      "Epoch : 2 - val_loss: 0.7662, val_acc: 0.6748 - ba: 0.5046\n",
      "\n",
      "Epoch : 2 - test_acc: 0.7896 - ba_test: 0.5275\n",
      "\n",
      "Epoch [3/10000], Step [2461/11392] Loss: 0.3624217510\n",
      "Epoch [3/10000], Step [4922/11392] Loss: 0.3479746878\n",
      "Epoch [3/10000], Step [7383/11392] Loss: 0.2962770164\n",
      "Epoch [3/10000], Step [9844/11392] Loss: 0.2884314358\n",
      "Epoch [3/10000], Loss_cc: 0.3497, Acc_cc: 0.8213\n",
      "Epoch : 3 - val_loss: 0.7518, val_acc: 0.6994 - ba: 0.5061\n",
      "\n",
      "Epoch : 3 - test_acc: 0.8263 - ba_test: 0.5003\n",
      "\n",
      "Epoch [4/10000], Step [2461/11392] Loss: 0.3641290665\n",
      "Epoch [4/10000], Step [4922/11392] Loss: 0.3395428658\n",
      "Epoch [4/10000], Step [7383/11392] Loss: 0.3197923303\n",
      "Epoch [4/10000], Step [9844/11392] Loss: 0.3288347423\n",
      "Epoch [4/10000], Loss_cc: 0.3405, Acc_cc: 0.8264\n",
      "Epoch : 4 - val_loss: 0.7493, val_acc: 0.6923 - ba: 0.5064\n",
      "\n",
      "Epoch : 4 - test_acc: 0.8273 - ba_test: 0.4991\n",
      "\n",
      "Epoch [5/10000], Step [2461/11392] Loss: 0.3139272034\n",
      "Epoch [5/10000], Step [4922/11392] Loss: 0.3659146726\n",
      "Epoch [5/10000], Step [7383/11392] Loss: 0.2500751615\n",
      "Epoch [5/10000], Step [9844/11392] Loss: 0.3208927512\n",
      "Epoch [5/10000], Loss_cc: 0.3310, Acc_cc: 0.8318\n",
      "Epoch : 5 - val_loss: 0.7623, val_acc: 0.6850 - ba: 0.5106\n",
      "\n",
      "Epoch : 5 - test_acc: 0.8007 - ba_test: 0.5217\n",
      "\n",
      "Epoch [6/10000], Step [2461/11392] Loss: 0.2947776318\n",
      "Epoch [6/10000], Step [4922/11392] Loss: 0.2657661736\n",
      "Epoch [6/10000], Step [7383/11392] Loss: 0.2995651960\n",
      "Epoch [6/10000], Step [9844/11392] Loss: 0.3415313065\n",
      "Epoch [6/10000], Loss_cc: 0.3233, Acc_cc: 0.8362\n",
      "Epoch : 6 - val_loss: 0.7858, val_acc: 0.7018 - ba: 0.5037\n",
      "\n",
      "Epoch : 6 - test_acc: 0.8135 - ba_test: 0.4937\n",
      "\n",
      "Epoch [7/10000], Step [2461/11392] Loss: 0.2513955235\n",
      "Epoch [7/10000], Step [4922/11392] Loss: 0.2943119407\n",
      "Epoch [7/10000], Step [7383/11392] Loss: 0.2916529775\n",
      "Epoch [7/10000], Step [9844/11392] Loss: 0.2505030334\n",
      "Epoch [7/10000], Loss_cc: 0.3162, Acc_cc: 0.8403\n",
      "Epoch : 7 - val_loss: 0.7603, val_acc: 0.7048 - ba: 0.5037\n",
      "\n",
      "Epoch : 7 - test_acc: 0.8438 - ba_test: 0.5094\n",
      "\n",
      "Epoch [8/10000], Step [2461/11392] Loss: 0.2894649208\n",
      "Epoch [8/10000], Step [4922/11392] Loss: 0.2732893229\n",
      "Epoch [8/10000], Step [7383/11392] Loss: 0.2692354023\n",
      "Epoch [8/10000], Step [9844/11392] Loss: 0.3870500028\n",
      "Epoch [8/10000], Loss_cc: 0.3093, Acc_cc: 0.8437\n",
      "Epoch : 8 - val_loss: 0.7828, val_acc: 0.6980 - ba: 0.5042\n",
      "\n",
      "Epoch : 8 - test_acc: 0.8103 - ba_test: 0.5164\n",
      "\n",
      "Epoch [9/10000], Step [2461/11392] Loss: 0.2846760452\n",
      "Epoch [9/10000], Step [4922/11392] Loss: 0.2593713403\n",
      "Epoch [9/10000], Step [7383/11392] Loss: 0.2825354636\n",
      "Epoch [9/10000], Step [9844/11392] Loss: 0.2408030331\n",
      "Epoch [9/10000], Loss_cc: 0.3033, Acc_cc: 0.8466\n",
      "Epoch : 9 - val_loss: 0.8420, val_acc: 0.6815 - ba: 0.5051\n",
      "\n",
      "Epoch : 9 - test_acc: 0.8191 - ba_test: 0.5175\n",
      "\n",
      "Epoch [10/10000], Step [2461/11392] Loss: 0.3668500483\n",
      "Epoch [10/10000], Step [4922/11392] Loss: 0.2434050888\n",
      "Epoch [10/10000], Step [7383/11392] Loss: 0.3165735602\n",
      "Epoch [10/10000], Step [9844/11392] Loss: 0.2016216815\n",
      "Epoch [10/10000], Loss_cc: 0.2975, Acc_cc: 0.8501\n",
      "Epoch : 10 - val_loss: 0.8099, val_acc: 0.6893 - ba: 0.5041\n",
      "\n",
      "Epoch : 10 - test_acc: 0.8001 - ba_test: 0.5006\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 66\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m acc \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     69\u001b[0m epoch_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader_1)\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/adamw.py:137\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/2023/lib/python3.9/site-packages/torch/optim/_functional.py:132\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    131\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 132\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "# 定义丢失函数和优化器\n",
    "#w = torch.Tensor([0.22, 0.365, 0.415])  \n",
    "w = torch.Tensor([0.1,0.9]) \n",
    "# 设定阴性和阳性的惩罚比例，这里是胡乱取的数\n",
    "criterion = nn.CrossEntropyLoss(w).to(device)\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=3e-4, weight_decay = 0.05, eps = 1e-4, betas = [0.9, 0.95])\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "# 原始文献lr=1e-4，\n",
    "# lucidrains为3e-5，且使用了scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "# 生成输出文件\n",
    "fconv = open(os.path.join(\"/data2/patho-vit_5_23_ccsurv/low/4/convergence.csv\"), \"w\")\n",
    "fconv.write(\"epoch, metric, value\\n\")\n",
    "fconv.close()\n",
    "\n",
    "# 正式训练和验证\n",
    "# 此处杀bug的重要心得：若出现TypeError: cannot convert the series to <class 'float'>，\n",
    "# 表示生成的pathids_train_1024.db，或pathids_test_1024.db里有空缺的标签值，可通过\n",
    "# 同名的xlsx进行查看。此处发现“202017947.A3.bif”切片未生成标签值，遂直接删除.\n",
    "#best_or = 1\n",
    "best_ba = 0\n",
    "#global best_or\n",
    "global best_ba\n",
    "\n",
    "#total_step_0 = len(train_loader_0)\n",
    "total_step_1 =  len(train_loader_1)\n",
    "# 开始训练和验证\n",
    "for epoch in range(epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_accuracy_lung = 0\n",
    "    epoch_loss_lung = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    #for i, (images, labels) in enumerate(train_loader_0):\n",
    "        #torch.cuda.empty_cache()\n",
    "    #    images = images.to(device)\n",
    "    #    labels = labels.to(device)\n",
    "    #    outputs, _ = vit(images)\n",
    "    #    loss_lung = criterion(outputs, labels.long())\n",
    "        \n",
    "    #    optimizer.zero_grad()\n",
    "    #    loss_lung.backward()\n",
    "    #    optimizer.step()\n",
    "        \n",
    "    #    acc_lung = (outputs.argmax(dim=-1) == labels).float().mean()\n",
    "    #    epoch_accuracy_lung += acc_lung / len(train_loader_0)\n",
    "    #    epoch_loss_lung += loss_lung / len(train_loader_0)\n",
    "        \n",
    "    #    if (i + 1) % 2558 == 0:\n",
    "        #if (i + 1) % 5230 == 0:\n",
    "    #        print(\"Epoch [{}/{}], Step [{}/{}] Loss_lung: {:.10f}\"\n",
    "    #             .format(epoch+1, epochs, i+1, total_step_0, loss_lung))\n",
    "            \n",
    "    for i, (images, labels) in enumerate(train_loader_1):\n",
    "        #torch.cuda.empty_cache()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, _ = vit(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (outputs.argmax(dim=-1) == labels).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader_1)\n",
    "        epoch_loss += loss / len(train_loader_1)\n",
    "        \n",
    "        if (i + 1) % 2461 == 0:\n",
    "        #if (i + 1) % 5230 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.10f}\"\n",
    "                 .format(epoch+1, epochs, i+1, total_step_1, loss))\n",
    "    \n",
    "    print(\"Epoch [{}/{}], Loss_cc: {:.4f}, Acc_cc: {:.4f}\".format(epoch+1, epochs, epoch_loss, epoch_accuracy))\n",
    "    \n",
    "    fconv = open(os.path.join(\"/data2/patho-vit_5_23_ccsurv/low/4/convergence.csv\"), \"a\")\n",
    "    #fconv.write(\"{}, loss_lung, {:.4f}\\n\".format(epoch+1, epoch_loss_lung))\n",
    "    #fconv.write(\"{}, acc_lung, {:.4f}\\n\".format(epoch+1, epoch_accuracy_lung))\n",
    "    fconv.write(\"{}, loss, {:.4f}\\n\".format(epoch+1, epoch_loss))\n",
    "    fconv.write(\"{}, acc, {:.4f}\\n\".format(epoch+1, epoch_accuracy))\n",
    "    fconv.close()\n",
    "        \n",
    "    if (epoch+1) % test_every == 0:\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            \n",
    "            pred = []\n",
    "            real = []\n",
    "            \n",
    "            for images, labels in valid_loader:\n",
    "                #torch.cuda.empty_cache()\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs, _ = vit(images)\n",
    "                val_loss = criterion(outputs, labels.long())\n",
    "                \n",
    "                #outputs = outputs.softmax(dim = -1)\n",
    "                outputs = outputs.argmax(dim = -1)\n",
    "                \n",
    "                acc = (outputs == labels).float().mean()\n",
    "                epoch_val_accuracy += acc / len(valid_loader)\n",
    "                epoch_val_loss += val_loss / len(valid_loader)\n",
    "            \n",
    "                pred.extend(list(outputs.cpu().numpy()))\n",
    "                real.extend(list(labels.cpu().numpy()))\n",
    "                \n",
    "            #df_pred = pd.DataFrame(pred)\n",
    "            #df_pred.to_csv(\"output_downstream/pred_real/pred_.csv\")\n",
    "            \n",
    "            #df_real = pd.DataFrame(real)\n",
    "            #df_real.to_csv(\"output_downstream/real.csv\")\n",
    "            \n",
    "            pred = np.array(pred)\n",
    "            real = np.array(real)\n",
    "            ba = balanced_accuracy_score(real, pred)\n",
    "            \n",
    "            # del pred real\n",
    "            # gc.collect()\n",
    "            \n",
    "            #c={\"prediction\": pred, \"labels\": real}\n",
    "            #df_c = pd.DataFrame(c)\n",
    "            #df_c.to_csv(\"output_jepa_label/pred_real/pred_{}.csv\".format(epoch+1))\n",
    "            \n",
    "            #neq = np.not_equal(pred, real)\n",
    "            #acc = 1 - float(neq.sum()) / pred.shape[0]\n",
    "            \n",
    "            eq = np.equal(pred, real)\n",
    "            sensi = float(np.logical_and(pred==1, eq).sum()) / (real==1).sum()\n",
    "            speci = float(np.logical_and(pred==0, eq).sum()) / (real==0).sum()\n",
    "            \n",
    "            #fpr = float(np.logical_and(pred==1, neq).sum()) / (real==0).sum()\n",
    "            #fnr = float(np.logical_and(pred==0, neq).sum()) / (real==1).sum()\n",
    "            \n",
    "            #odds_ratio = (float(np.logical_and(pred==1, eq).sum()) * float(np.logical_and(pred==0, eq).sum())) \\\n",
    "            #            / (float(np.logical_and(pred==0, neq).sum()) * float(np.logical_and(pred==1, neq).sum()) + 1e-9)\n",
    "            \n",
    "            fconv = open(os.path.join(\"/data2/patho-vit_5_23_ccsurv/low/4/convergence.csv\"), \"a\")\n",
    "            fconv.write(\"{}, epoch_val_loss, {:.4f}\\n\".format(epoch+1, epoch_val_loss))\n",
    "            fconv.write(\"{}, val_acc, {:.4f}\\n\".format(epoch+1, epoch_val_accuracy))\n",
    "            fconv.write(\"{}, ba, {:.4f}\\n\".format(epoch+1, ba))\n",
    "            fconv.write(\"{}, sensi, {:.4f}\\n\".format(epoch+1, sensi))\n",
    "            fconv.write(\"{}, speci, {:.4f}\\n\".format(epoch+1, speci))\n",
    "            #fconv.write(\"{}, odds_ratio, {:.4f}\\n\".format(epoch+1, odds_ratio))\n",
    "            #fconv.write(\"{}, fpr, {}\\n\".format(epoch+1, fpr))\n",
    "            #fconv.write(\"{}, fnr, {}\\n\".format(epoch+1, fnr))\n",
    "            fconv.close()\n",
    "\n",
    "\n",
    "            epoch_test_accuracy = 0\n",
    "            #epoch_test_loss = 0\n",
    "            \n",
    "            pred_test = []\n",
    "            real_test = []\n",
    "            \n",
    "            for images, labels in valid_loader_1:\n",
    "                #torch.cuda.empty_cache()\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs, _ = vit(images)\n",
    "                #test_loss = criterion(outputs, labels.long())\n",
    "                \n",
    "                #outputs = outputs.softmax(dim = -1)\n",
    "                outputs = outputs.argmax(dim = -1)\n",
    "                \n",
    "                test_acc = (outputs == labels).float().mean()\n",
    "                epoch_test_accuracy += test_acc / len(valid_loader_1)\n",
    "                #epoch_test_loss += val_loss / len(valid_loader_1)\n",
    "            \n",
    "                pred_test.extend(list(outputs.cpu().numpy()))\n",
    "                real_test.extend(list(labels.cpu().numpy()))\n",
    "                \n",
    "            #df_pred = pd.DataFrame(pred)\n",
    "            #df_pred.to_csv(\"output_downstream/pred_real/pred_.csv\")\n",
    "            \n",
    "            #df_real = pd.DataFrame(real)\n",
    "            #df_real.to_csv(\"output_downstream/real.csv\")\n",
    "            \n",
    "            pred_test = np.array(pred_test)\n",
    "            real_test = np.array(real_test)\n",
    "            ba_test = balanced_accuracy_score(real_test, pred_test)\n",
    "            \n",
    "            # del pred real\n",
    "            # gc.collect()\n",
    "            \n",
    "            #c={\"prediction\": pred, \"labels\": real}\n",
    "            #df_c = pd.DataFrame(c)\n",
    "            #df_c.to_csv(\"output_jepa_label/pred_real/pred_{}.csv\".format(epoch+1))\n",
    "            \n",
    "            #neq = np.not_equal(pred, real)\n",
    "            #acc = 1 - float(neq.sum()) / pred.shape[0]\n",
    "            \n",
    "            eq_test = np.equal(pred_test, real_test)\n",
    "            sensi_test = float(np.logical_and(pred_test==1, eq_test).sum()) / (real_test==1).sum()\n",
    "            speci_test = float(np.logical_and(pred_test==0, eq_test).sum()) / (real_test==0).sum()\n",
    "            \n",
    "            #fpr = float(np.logical_and(pred==1, neq).sum()) / (real==0).sum()\n",
    "            #fnr = float(np.logical_and(pred==0, neq).sum()) / (real==1).sum()\n",
    "            \n",
    "            #odds_ratio = (float(np.logical_and(pred==1, eq).sum()) * float(np.logical_and(pred==0, eq).sum())) \\\n",
    "            #            / (float(np.logical_and(pred==0, neq).sum()) * float(np.logical_and(pred==1, neq).sum()) + 1e-9)\n",
    "            \n",
    "            fconv = open(os.path.join(\"/data2/patho-vit_5_23_ccsurv/low/4/convergence.csv\"), \"a\")\n",
    "            #fconv.write(\"{}, epoch_val_loss, {:.4f}\\n\".format(epoch+1, epoch_val_loss))\n",
    "            fconv.write(\"{}, test_acc, {:.4f}\\n\".format(epoch+1, epoch_test_accuracy))\n",
    "            fconv.write(\"{}, ba_test, {:.4f}\\n\".format(epoch+1, ba_test))\n",
    "            fconv.write(\"{}, sensi_test, {:.4f}\\n\".format(epoch+1, sensi_test))\n",
    "            fconv.write(\"{}, speci_test, {:.4f}\\n\".format(epoch+1, speci_test))\n",
    "            #fconv.write(\"{}, odds_ratio, {:.4f}\\n\".format(epoch+1, odds_ratio))\n",
    "            #fconv.write(\"{}, fpr, {}\\n\".format(epoch+1, fpr))\n",
    "            #fconv.write(\"{}, fnr, {}\\n\".format(epoch+1, fnr))\n",
    "            fconv.close()\n",
    "            \n",
    "    torch.save(vit.state_dict(), os.path.join(\"/data2/patho-vit_5_23_ccsurv/low/4/12.11/checkpoint_ccsurv_{}.pth\".format(epoch+1)))\n",
    "        \n",
    "    #if ba >= best_ba:\n",
    "        # 或用 if ba >= best_ba:\n",
    "        #    best_or = odds_ratio\n",
    "    #    best_ba = ba\n",
    "        \n",
    "    #torch.save(vit.state_dict(), os.path.join(\"/data2/patho-vit_5_23_ccim/low/4-13waibu/best_checkpoint_2classes_{}.pth\".format(epoch+1)))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - val_loss: {epoch_val_loss:.4f}, val_acc: {epoch_val_accuracy:.4f} - ba: {ba:.4f}\\n\" \n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - test_acc: {epoch_test_accuracy:.4f} - ba_test: {ba_test:.4f}\\n\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8d41b-067f-4e10-b781-c748ca25f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354fb17-68be-4f8b-b8f1-11f11462b90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818b1f3-e2be-4c3a-ae7b-39d2ec8c008d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ca606-87a2-4a22-9f55-dadd275d9d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c4626-972f-4dba-acbb-bbfc2631bb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2182c81-3958-4cb9-b43c-2a4333158746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机场2，为每个补丁抽出特征，为整张切片制作特征图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ceea6c4-0578-47bd-8bfd-8aaf4282324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = \"/data2/patho-vit_5_23_ccsurv/low\"\n",
    "# 此处patho_vit后的_5_23为包的版本号\n",
    "# 若此jupyternotebook运行中kernel挂掉，重启后仅需运行此一代码块，然后跳到需要运行的代码块即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75403381-e96f-4c6f-b6f1-52d7289e57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5164cf96-ccbb-43c1-908d-3e8c943e57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_vit.vit_luci2 import ViT as ViT\n",
    "from patho_vit.airport2 import train_features_extractor_gpvit2 as Extractor\n",
    "from patho_vit.airport2 import valid_features_extractor_gpvit2 as Extractor2\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52aff67c-a92e-4fb0-b6d7-a4d9bffb0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extractor3(path, train_lib, transform, batch_size, model, device):\n",
    "    #model = model.to(device)\n",
    "    #model = nn.DataParallel(model)\n",
    "    model.eval()\n",
    "\n",
    "    for i, image in enumerate(train_lib[\"images\"]):\n",
    "        cols = []\n",
    "        rows=[]\n",
    "\n",
    "        for patch in train_lib[\"images\"][i]:\n",
    "            col, row = patch.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "            cols.append(int(col))\n",
    "            rows.append(int(row))\n",
    "        col_last = sorted(np.array(cols))[-1]\n",
    "        row_last = sorted(np.array(rows))[-1]\n",
    "        image = torch.zeros((row_last+1), (col_last+1), 768)\n",
    "    \n",
    "        patches_batch = []  # 摆渡车\n",
    "        patches_pos = []  # 补丁位置\n",
    "        img_features =[]\n",
    "        for patch in train_lib[\"images\"][i]:\n",
    "            col, row = patch.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "            patches_pos.append([int(col), int(row)])\n",
    "    \n",
    "        for j, patch in enumerate(train_lib[\"images\"][i]):\n",
    "            torch.cuda.empty_cache() \n",
    "            #patch = cv2.imread(patch)\n",
    "            #patch = cv2.resize(patch, (384, 384))\n",
    "            #patch = cv2.cvtColor(patch, cv2.COLOR_BGR2RGB)\n",
    "            patch = Image.open(patch)\n",
    "\n",
    "            patch = transform(patch).unsqueeze(0) \n",
    "            patches_batch.append(patch)\n",
    "        \n",
    "            if ((j+1) % batch_size == 0) or ((j+1) == len(train_lib[\"images\"][i])):\n",
    "                torch.cuda.empty_cache()  \n",
    "                patches_batch = torch.cat(patches_batch, 0).to(device)\n",
    "                _, features = model(patches_batch)\n",
    "                #features = features.softmax(dim = -1)\n",
    "                features = features.cpu().tolist()\n",
    "                for k in range(len(features)):\n",
    "                    img_features.append(features[k])\n",
    "                patches_batch = []  # 清空摆渡车\n",
    "    \n",
    "        for index in range(len(patches_pos)):\n",
    "            batch_range = torch.arange(patches_pos[index][1], patches_pos[index][1] + 1)[:, None]\n",
    "            indices = torch.arange(patches_pos[index][0], patches_pos[index][0] + 1)\n",
    "            image[batch_range, indices] = torch.Tensor(img_features[index]).reshape(1, 1, 768)\n",
    "    \n",
    "        #train_lib[\"images\"][i] = image\n",
    "\n",
    "        torch.save(image, \"store_5_96/{}.db\".format(train_lib[\"pathid\"][i]))\n",
    "\n",
    "    images = []\n",
    "    for i, pathid in enumerate(train_lib[\"pathid\"]):\n",
    "        images.append(os.path.join(path, \"store_5_96\", pathid+\".db\"))\n",
    "    c = {\"pathid\": train_lib[\"pathid\"], \"images\": images, \"labels\": train_lib[\"labels\"]}\n",
    "    df = pd.DataFrame(c)\n",
    "    df.to_csv(\"files_label/store_5_96_exva.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "    torch.save(c, \"files_label/store_5_96_exva.db\")\n",
    "    return \"store_5_96_exva.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc3a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    image_size = 384,\n",
    "    patch_size = 16,\n",
    "    dim = 768,\n",
    "    depth = 12,\n",
    "    heads = 12,\n",
    "    mlp_dim = 768 * 4,\n",
    "    num_classes = 2\n",
    ")\n",
    "#output, feature = vit(torch.randn(2, 3, 384, 384))\n",
    "#feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f70c2f-72a2-4155-8fc6-b3abaa0070a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入定制vit，加载目标编码器的权重\n",
    "\n",
    "#weights = torch.load(\"output_jepa/gpvit_weight_4_22_epoch_1.pt\")\n",
    "weights = torch.load(\"/data2/patho-vit_5_23_ccsurv/low/4/12.7/checkpoint_ccsurv_2.pth\")\n",
    "\n",
    "vit.to(device)\n",
    "vit = nn.DataParallel(vit)\n",
    "vit.load_state_dict(weights, strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dde870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个文件夹一个文件夹的抽特征，并重构（384，384，768）的图像\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize([384, 384]),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "#train_lib = \"files/store_5_train_{}.db\".format((n+1) * 25)\n",
    "train_lib = torch.load(\"/data2/patho-vit_5_23_ccsurv/files_label/store_4_train_ccsurv.db\")\n",
    "valid_lib = torch.load(\"/data2/patho-vit_5_23_ccsurv/files_label/store_4_valid_ccsurv.db\")\n",
    "exva_lib = torch.load(\"/data2/patho-vit_5_23_ccsurv/SZL_exva/files/store_4.db\")\n",
    "\n",
    "extractor = Extractor(path = path, train_lib = train_lib, transform = transform, batch_size = 200, model = vit, device = device)\n",
    "extractor2 = Extractor2(path = path, train_lib = valid_lib, transform = transform, batch_size = 200, model = vit, device = device)\n",
    "extractor3 = Extractor3(path = path, train_lib = exva_lib, transform = transform, batch_size = 200, model = vit, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4641f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bdd09e-6201-475b-a036-b2793a111e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbc241-a844-4f1a-a6c4-b9623b852b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3442bfd-35cc-4b63-8398-275e24f65cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c425b6f-54c6-4551-a0b0-59addd621620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023",
   "language": "python",
   "name": "2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
